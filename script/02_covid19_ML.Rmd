---
title: "Exploring Mick Eran et al (2022) gene expression data for COVID-19 prediction"
author: Lakhansing Pardeshi, Wageningen University & Research
output: html_document
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
---

Eran et. al. examined the transcriptional response among patients with acute respiratory illness (ARI) to identify gene expression signatures unique to COVID-19 infection. For this study, upper airway transcriptional data was collected from COVID-19 infected patients (n=93) and other patients with viral (n=43) or non-viral (n=100) ARI. The authors built a machine learning model by employing a combination of lasso regularized regression and random forest. They identified 27 gene signatures that classified the COVID-19 infected patients from other ARIs with an area under the receiver operating characteristic curve (AUROC) of 0.98.

I have explored this transcriptomics data using principal component analysis (PCA). Subsequently, I applied multiple combinations of feature selection and machine learning (ML) algorithms on this data and compared these results with Eran et. al.

This analysis was performed using R and all the scripts are available on GitHub page https://github.com/lakhanp1/covid19_Mick_et_al.ML. PCA exploration of data was performed using `factoextra` and `FactoMineR` packages. `tidymodels`, a framework based on `tidyverse` principles was used for ML analysis. This notebook describes the analysis steps for this ML task.


```{r setup}
knitr::opts_chunk$set(
  echo = TRUE, results = 'markup', warning = FALSE, strip.white = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(DESeq2))
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(matrixStats))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(FactoMineR))
suppressPackageStartupMessages(library(recipeselectors))
suppressPackageStartupMessages(library(FSelectorRcpp))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(nomnoml))


rm(list = ls())

set.seed(124)

file_geneCounts <- here::here("data", "swab_gene_counts.csv")
file_meta <- here::here("data", "metatable_with_viral_status.csv")
file_gene2Name <- here::here("data/annotation", "gene2name.txt")

pt_theme <- theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 12, color = "black"),
    plot.title = element_text(face = "bold", size = 14, color = "black"),
    axis.title = element_text(face = "bold", size = 13, color = "black")
  )

cores <- parallel::detectCores()
cores <- 4

# cl <- makePSOCKcluster(cores)
# doParallel::registerDoParallel(cl)
# stopCluster(cl)

```

## Preprocessing

```{r}

sampleInfo <- suppressMessages(readr::read_csv(file = file_meta)) %>% 
  dplyr::rename(sampleId = CZB_ID) %>% 
  dplyr::mutate(
    class = dplyr::case_when(
      viral_status == "SC2" ~ "positive",
      viral_status == "no_virus" ~ "negative",
      viral_status == "other_virus" ~ "negative",
      TRUE ~ NA_character_
    ),
    viral_status = forcats::fct_relevel(
      .f = viral_status, "SC2", "no_virus", "other_virus"
    ),
    class = forcats::fct_relevel(
      .f = class, "positive", "negative"
    )
  )

metadataCols <- setdiff(colnames(sampleInfo), "class")

covidData <- suppressMessages(readr::read_csv(file = file_geneCounts)) %>% 
  dplyr::rename(geneId = "...1") %>% 
  tidyr::pivot_longer(cols = !geneId, names_to = "sampleId", values_to = "count") %>% 
  tidyr::pivot_wider(names_from = geneId, values_from = count) %>% 
  dplyr::left_join(y = sampleInfo, by = "sampleId") %>% 
  dplyr::select(class, !!!metadataCols, everything())

gene2Name <- suppressMessages(
  readr::read_tsv(
    file = file_gene2Name, col_names = c("geneId", "geneName"), col_select = 1:2
  )
)

nFeatures <- ncol(covidData) - ncol(sampleInfo)
nSamples <- nrow(covidData)

```

The current dataset has `r length(unique(sampleInfo$viral_status))` categories of
patients. 

```{r}
levels(sampleInfo$viral_status)
```

However, to simplify the prediction task, `no_virus` and `other_virus`
categories were merged into `negative` category and `SC2` category samples are used
as `positive` category samples.

```{r}
levels(sampleInfo$class)
```

### Data summary

```{r}

pt_stats <- ggplot(
  data = sampleInfo,
  mapping = aes(x = viral_status, fill = gender)
) +
  geom_bar(
    stat = "count"
  ) +
  geom_text(
    mapping = aes(label = ..count..), stat = "count",
    position = position_stack(vjust = 0.5),
    color = "black", size = 6, fontface = "bold"
  ) +
  labs(title = "Class distribution of data") +
  scale_fill_manual(
    values = c("F" = "#F06C9B", "M" = "#61A0AF")
  ) +
  scale_x_discrete(
    labels = dplyr::count(sampleInfo, viral_status) %>% 
      dplyr::mutate(label = paste(viral_status, "\n(", n, ")", sep = "")) %>% 
      dplyr::pull(var = label, name = viral_status)
  ) +
  pt_theme

pt_stats

```

### Normalize raw counts: DESeq2 `vst()`

```{r, results='hide'}
countMat <- dplyr::select(covidData, -class, -!!metadataCols) %>% 
  as.matrix()

rownames(countMat) <- covidData$sampleId

normCountMat <- DESeq2::varianceStabilizingTransformation(t(countMat))
```

### Exploring data using PCA

For PCA analysis, top 2000 genes showing highest variation are selected.

```{r}
## select top 2000 rows/genes with highest variation
rv <- matrixStats::rowVars(normCountMat)
keep <- order(rv, decreasing=TRUE)[seq_len(min(2000, length(rv)))]

## remove low count rows
# keep<- rowSums(normCountMat > 1) >= 2

normCountMatFiltered <- normCountMat[keep, ]

pcaData <- tibble::as_tibble(t(normCountMatFiltered), rownames = "sampleId") %>% 
  dplyr::left_join(y = sampleInfo, by = "sampleId") %>% 
  dplyr::select(class, !!!metadataCols, everything()) %>% 
  as.data.frame()

row.names(pcaData) <- pcaData$sampleId

res.pca <- FactoMineR::PCA(
  X = pcaData, graph = FALSE, scale.unit = TRUE,
  quali.sup = 1:ncol(sampleInfo), ncp = 10
)

eig.val <- factoextra::get_eigenvalue(res.pca)

## Graph of individuals
ind <- factoextra::get_pca_ind(res.pca)
var <- factoextra::get_pca_var(res.pca)
```

Scree plot showing percentage of the explained variance by top 10 PCs.

```{r}
## scree plot: variance by PC
factoextra::fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

First two PCs explain `r eig.val[2, "cumulative.variance.percent"]`% variance in
the data. Top 10 PCs explain `r eig.val[10, "cumulative.variance.percent"]`% variance
in the data. Out of `r nrow(eig.val)` PCs, 
`r length(which(eig.val[, "cumulative.variance.percent"] <= 80))` are required to
explain the 80% variance in the current data.

```{r}
as.data.frame(eig.val) %>% 
  tibble::rownames_to_column(var = "pc") %>% 
  dplyr::mutate(dim = 1:n()) %>% 
  dplyr::filter(cumulative.variance.percent <= 80) %>% 
  # dplyr::slice(1:20) %>%
  ggplot2::ggplot(
    mapping = aes(x = dim, y = cumulative.variance.percent)
  ) +
  geom_point() +
  geom_line() +
  labs(
    x = "Dimensions",
    y = "% variance explained by PCs",
    title = "Cumulative variance of PCs"
  ) +
  pt_theme
```

Projection of samples on PC1 and PC2 does not result into clear clustering of the patients with respect viral status.

```{r}
factoextra::fviz_pca_ind(
  X = res.pca,
  fill.ind = sampleInfo$viral_status,
  pointshape = 21,
  repel = TRUE,
  mean.point = FALSE,
  geom = c("point"),
  legend.title = "Study",
  pointsize = 2
)
```

Exploring additional PCs also leads to the same observations.

```{r}
## prepare the plot dataframe for ggplot
plotData <- as.data.frame(ind$coord) %>%
  tibble::rownames_to_column(var = "sampleId") %>%
  dplyr::left_join(y = sampleInfo, by = c("sampleId" = "sampleId"))

pairs(
  x = plotData[, 2:6],
  pch = 19,  cex = 1,
  col = plotData$viral_status,
  lower.panel=NULL
)
```

Based on the above results, unsupervised linear feature extraction using PCA is not sufficient to cluster the samples into different categories. However, a ML algorithm can still use the features from PCA to identify a non-linear decision boundry in two classes. In the next steps, we will attempt to use the PCA as one of the feature extraction methods for building prediction model.

## Machine Learning setup

### Data processing flowchart

Figure below explains the ML workflow.

```{nomnoml fig.height=3, fig.width=8, out.width="100%"}
#stroke: orange
#fontSize: 18
#lineWidth: 4
[raw counts]->80%[modelling]
[raw counts]->20%[holdout]
[modelling]->[Cross Validation]
[Cross Validation|fold1|fold2|fold3|fold5|fold5]->[DESeq2::vst() normalization]
[DESeq2::vst() normalization]->[PCA]
[DESeq2::vst() normalization]->[Information gain]
[PCA]->[ML | Random Forest | SVM | Lasso]
[Information gain]->[ML | Random Forest | SVM | Lasso]
[ML]->[optimized model]
[optimized model]->[holdout]
[holdout]->[final evaluation]

```

### Data splitting and resampling

20% data is hold outside and 80% data will be used for modelling purpose using 5
fold crossvalidation. This holdout dataset will be used to evaluate the final
optimized model. Optimized models of multiple learners will be evaluated against
this holdout set to select the best performing learner.

```{r}
splits <- rsample::initial_split(data = covidData, prop = 4/5, strata = viral_status)

dataHoldout <- rsample::testing(splits)
dataModelling <- rsample::training(splits)

folds <- rsample::vfold_cv(
  data = dataModelling, v = 5, strata = viral_status
)

# rsample::analysis(x = folds$splits[[1]])
# rsample::assessment(x = folds$splits[[1]])

```

### Variance stalilizing transformation (VST) from DESeq2 package as a recipe

Gene expression raw counts will be normalized using variance stabilizing transformation (VST) methods of `DESeq2` package. This transformation will result into library size normalized count matrix with constant variance along the range of mean values. During cross-validation step, VST transformation will be applied to the training set. Dispersion estimates for Negative Binomial distributed training data will be calculated using `DESeq2::estimateDispersions()` function and these dispersions will be applied to the test data. Please refer to `DESeq2` manual for additional details.


To process the training data for each fold and its respective test fold, we will build a custom VST normalization recipe. The following code block constructs `step_deseq2_vst` recipe. This recipe will be integrated in ML modelling workflow.

```{r}
## Create the function
step_deseq2_vst <- function(
    recipe, 
    ..., 
    role = NA,
    trained = FALSE, 
    dispersionFn = NULL,
    geneIds = NULL,
    options = list(blind = TRUE, fitType = "parametric"),
    skip = FALSE,
    id = recipes::rand_id("DESeq2_vst")
) {
  
  # dispersionFn: this variable will store the dispersion function calculated
  # from the training set in prep() and will be applied to new dataset in bake()
  
  ## The variable selectors are not immediately evaluated by using
  ##  the `quos()` function in `rlang`. `ellipse_check()` captures 
  ##  the values and also checks to make sure that they are not empty.  
  terms <- ellipse_check(...) 
  
  recipes::add_step(
    recipe, 
    step_deseq2_vst_new(
      terms = terms, 
      trained = trained,
      role = role, 
      dispersionFn = dispersionFn,
      geneIds = geneIds,
      options = options,
      skip = skip,
      id = id
    )
  )
  
}

## Initialize a new object
step_deseq2_vst_new <- function(terms, role, trained, dispersionFn, geneIds, options, skip, id) {
  recipes::step(
    subclass = "deseq2_vst", 
    terms = terms,
    role = role,
    trained = trained,
    dispersionFn = dispersionFn,
    geneIds = geneIds,
    options = options,
    skip = skip,
    id = id
  )
}


## Create the prep method
#' @export
prep.step_deseq2_vst <- function(x, training, info = NULL, ...) {
  ## Remember that the prep() function does not apply the step to the data;
  ## it only estimates any required values 
  col_names <- recipes::recipes_eval_select(x$terms, training, info)
  recipes::check_type(training[, col_names])
  
  ## use col_names to ensure the same column order for the data
  ## compute the dispersion function from the training data
  dds_train <- DESeq2::DESeqDataSetFromMatrix(
    countData = as.matrix(t(training[, col_names])),
    colData = DataFrame(condition = rep("A", nrow(training))),
    design = ~1
  )
  dds_train <- DESeq2::estimateSizeFactors(dds_train)
  dds_train <- DESeq2::estimateDispersions(dds_train)
  
  dispersionFn <- DESeq2::dispersionFunction(dds_train)
  
  ## Use the constructor function to return the updated object. 
  ## Note that `trained` is now set to TRUE
  ## dispersionFn: has the dispersion function from training data
  step_deseq2_vst_new(
    terms = x$terms,
    role = x$role,
    trained = TRUE,
    dispersionFn = dispersionFn,
    geneIds = col_names,
    options = x$options,
    skip = x$skip,
    id = x$id
  )
}


## Create the bake method
bake.step_deseq2_vst <- function(object, new_data, ...) {
  # object: updated step function that has been through the corresponding prep() code
  # new_data: tibble of data to be processed. 
  # return: a tibble of the modified version of new_data
  
  ## use col_names to ensure the same column order for the data
  col_names <- object$geneIds
  
  recipes::check_new_data(col_names, object, new_data)
  
  dds_newData <- DESeq2::DESeqDataSetFromMatrix(
    countData = as.matrix(t(new_data[, col_names])),
    colData = DataFrame(condition = rep("A", nrow(new_data))),
    design = ~1
  )
  
  dds_newData <- DESeq2::estimateSizeFactors(dds_newData)
  
  ## apply dispersion function from obtained from the training data
  DESeq2::dispersionFunction(dds_newData) <- object$dispersionFn
  
  ## normalize the data
  vstCall <- expr(
    DESeq2::varianceStabilizingTransformation(
      object = dds_newData,
      !!!object$options
    )
  )
  
  vst <- eval(vstCall)
  
  normCounts <- tibble::as_tibble(t(assay(vst)))
  
  ## replace the count data with normalized data
  new_data[, col_names] <- normCounts[, col_names]
  
  new_data
}
```

### Data preprocessing setup

#### Define column roles and raw count normalization

A basic recipe is created which involves assigning the predictor and outcome roles
to the variables and counts data normalization using VST.

```{r}
## a utliity function to test if a recipe works as expected
check_if_recipe_cooks <- function(rcp){
  preped_rec <- recipes::prep(
    x = rcp,
    training = rsample::analysis(x = folds$splits[[1]])
  )
  
  test_baked <- recipes::bake(
    object = preped_rec,
    new_data = rsample::assessment(x = folds$splits[[1]])
  )
  
  return(test_baked)
}


## basic recipe for count normalization
rec_vst <- recipes::recipe(x = dataModelling) %>% 
  recipes::update_role(tidyselect::everything(), new_role = "predictor") %>% 
  recipes::update_role(class, new_role = "outcome") %>% 
  recipes::update_role(!!!syms(metadataCols), new_role = "ID") %>% 
  step_deseq2_vst(recipes::all_predictors())

# cooked_rcp <- check_if_recipe_cooks(rcp = rec_vst)
```

We will use one feature selection method, **information gain** and feature extraction method **PCA** for selecting best features for modelling. `rec_vst` recipe will be extended to include the feature selection step for each of these methods.

#### Feature selection recipe: information gain

Information gain calculates entropy of each feature with respect to class. For current
analysis, we will use top 50 most informative features for ML modelling.

```{r}
## feature selection recipe using information gain 
infogain_rec <- rec_vst %>% 
  recipeselectors::step_select_infgain(
    recipes::all_predictors(), outcome = "class",  
    type = "infogain", threads = !!cores, top_p = 50
  )

# cooked_rcp <- check_if_recipe_cooks(rcp = infogain_rec)
```

#### Feature extraction recipe: PCA

PCA will be used to extract features and principal components cumulatively explaining 80% variance
will be used as features in the subsequent ML modelling step.

```{r}
pca_rec <- rec_vst %>% 
  recipes::step_normalize(all_predictors()) %>% 
  recipes::step_pca(
    recipes::all_predictors(), threshold = 0.8
  )

# cooked_rcp <- check_if_recipe_cooks(rcp = pca_rec)
```


#### Other convenience functions

```{r}
## a generic function to
## 1) extract the best performing from hyperparameter tuning results
## 2) store the AUC data and hyperparameters for best model
## 3) fit the best model to initial_split and save performance for holdout test data 
fit_store_best_model <- function(
    tune_res, outPrefix, label, truth_col, pred_cols, wf = NULL, evalSplits = NULL
){
  ## select best
  bestModel <- tune::select_best(tune_res, metric = "roc_auc")
  readr::write_tsv(
    x = bestModel, file = paste(outPrefix, ".best_model.tsv", sep = "")
  )
  
  ## store AUC
  aucDf <- tune::collect_predictions(x = tune_res, parameters = bestModel) %>% 
    yardstick::roc_curve(!!truth_col, !!!pred_cols) %>%
    dplyr::mutate(model = label)
  
  readr::write_tsv(x = aucDf, file = paste(outPrefix, ".auc.tsv", sep = ""))
  
  finalFit <- NULL
  
  ## fit to initial_split
  if(!is.null(evalSplits) && !is.null(wf)){
    finalWf <- tune::finalize_workflow(x = wf, parameters = bestModel)
    finalFit <- tune::last_fit(object = finalWf, split = evalSplits)
    
    collect_metrics(lasso_pca_holdFit) %>% 
      readr::write_tsv(file = paste(outPrefix, ".final_prediction.tsv", sep = ""))
  }
  
  return(
    list(bestModel = bestModel, auc = aucDf, finalFit = finalFit)
  )
}

```


In the next steps, we will select the model and ML engine for prediction task. We will
be applying three ML methods on our data; **Random Forest**, **Support Vector Machines**
and **Logistic Regression with LASSO** and select the best method after performance evaluation.

### Random Forest

First, we will define the random forest model and hyperparameter tuning grid.

```{r}
## random forest model
rf_mod <- parsnip::rand_forest(
  trees = 10000,
  mtry = tune(), min_n = tune()
) %>% 
  parsnip::set_engine("ranger", num.threads = !!cores) %>% 
  parsnip::set_mode("classification")

## grid for optimum hyperparameter search
rf_grid <- dials::grid_regular(
  dials::mtry(range = c(1L, 10L)),
  dials::min_n(range = c(2L, 40L)),
  levels = 5
)

rf_mod
```

#### PCA with Random Forest

```{r eval=FALSE}
## random forest workflow
rf_pca_workflow <- workflows::workflow() %>% 
  workflows::add_model(rf_mod) %>% 
  workflows::add_recipe(pca_rec)

rf_pca_res <- rf_pca_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = rf_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(rf_pca_res)
# tune::show_best(rf_pca_res)

rf_pca_best <- fit_store_best_model(
  tune_res = rf_pca_res,
  outPrefix = "rf_pca", label = "PCA + Random Forest",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = rf_pca_workflow, evalSplits = splits
  )

```


#### Information gain with Random Forest 

```{r eval=FALSE}
## random forest workflow
rf_info_workflow <- workflows::workflow() %>% 
  workflows::add_model(rf_mod) %>% 
  workflows::add_recipe(infogain_rec)

rf_info_res <- rf_info_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = rf_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(rf_info_res)
# tune::show_best(rf_info_res)

## select hyperparameters of the best performing model
rf_info_best <- fit_store_best_model(
  tune_res = rf_info_res,
  outPrefix = "rf_info", label = "Information Gain + Random Forest",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = rf_info_workflow, evalSplits = splits
  )

```


### SVM

```{r}
## SVM model
svm_mod <- parsnip::svm_rbf(
  cost = tune(), rbf_sigma = tune()
) %>% 
  parsnip::set_engine("kernlab") %>% 
  parsnip::set_mode("classification")

## grid for optimum hyperparameter search
svm_grid <- dials::grid_regular(
  dials::cost(),
  dials::rbf_sigma(),
  levels = 5
)

svm_mod
```

#### PCA with SVM

```{r eval=FALSE}
## SVM workflow
svm_pca_workflow <- workflows::workflow() %>% 
  workflows::add_model(svm_mod) %>% 
  workflows::add_recipe(pca_rec)

svm_pca_res <- svm_pca_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = svm_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(svm_pca_res)
# tune::show_best(svm_pca_res)

## select hyperparameters of the best performing model
svm_pca_best <- fit_store_best_model(
  tune_res = svm_pca_res,
  outPrefix = "svm_pca", label = "PCA + SVM",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = svm_pca_workflow, evalSplits = splits
  )

```

#### Information gain with SVM

```{r eval=FALSE}
## SVM workflow
svm_info_workflow <- workflows::workflow() %>% 
  workflows::add_model(svm_mod) %>% 
  workflows::add_recipe(infogain_rec)

svm_info_res <- svm_info_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = svm_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(svm_info_res)
# tune::show_best(svm_info_res)

## select hyperparameters of the best performing model
svm_info_best <- fit_store_best_model(
  tune_res = svm_info_res,
  outPrefix = "svm_info", label = "Information Gain + SVM",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = svm_info_workflow, evalSplits = splits
  )

```


### Lasso logistic regression

```{r}
## logistic regression: Lasso
lasso_mod <- parsnip::logistic_reg(
  mixture = 1, penalty = tune()
) %>% 
  parsnip::set_engine("glmnet") %>% 
  parsnip::set_mode("classification")

## grid for optimum hyperparameter search
lasso_grid <- dials::grid_regular(
  dials::penalty(),
  levels = 10
)

lasso_mod
```


#### PCA with Lasso

``` {r eval=FALSE}
lasso_pca_workflow <- workflows::workflow() %>% 
  workflows::add_model(lasso_mod) %>% 
  workflows::add_recipe(pca_rec)

lasso_pca_res <- lasso_pca_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = lasso_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(lasso_pca_res)
# tune::show_best(lasso_pca_res)

## select hyperparameters of the best performing model
lasso_pca_best <- fit_store_best_model(
  tune_res = lasso_pca_res,
  outPrefix = "lasso_pca", label = "PCA + Lasso",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = lasso_pca_workflow, evalSplits = splits
  )

```


#### Information gain with Lasso

``` {r eval=FALSE}
lasso_info_workflow <- workflows::workflow() %>% 
  workflows::add_model(lasso_mod) %>% 
  workflows::add_recipe(infogain_rec)


lasso_info_res <- lasso_info_workflow %>% 
  tune::tune_grid(
    resamples = folds,
    grid = lasso_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )

# tune::collect_metrics(lasso_info_res)
# tune::show_best(lasso_info_res)

## select hyperparameters of the best performing model
lasso_info_best <- fit_store_best_model(
  tune_res = lasso_info_res,
  outPrefix = "lasso_info", label = "Information Gain + Lasso",
  truth_col = "class", pred_cols = ".pred_positive",
  wf = lasso_info_workflow, evalSplits = splits
  )

```


### Compare and summarize results

#### Cross validation performance

```{r echo=FALSE}
## compile best model summary during tuning
files_cv_res <- tibble::tibble(
  files = list.files(
    path = here::here("analysis/02_ML_models"),
    pattern = "*.best_model.tsv", full.names = TRUE
  )
) %>% 
  dplyr::mutate(
    id = stringr::str_replace(
      string = basename(files), pattern = ".best_model.tsv", replacement = "")
  ) %>% 
  purrr::pmap_dfr(
    # .x = files, .y = id,
    .f = function(files, id){
      readr::read_tsv(file = files, progress = FALSE, show_col_types = FALSE) %>% 
        dplyr::select(.metric, .estimator, mean, n, std_err) %>%
        dplyr::mutate(id = id)
    }
  ) %>% 
  tidyr::separate(col = id, into = c("algo", "fsel"), sep = "_") %>% 
  dplyr::mutate(
    algo = dplyr::case_when(
      algo == "lasso" ~ "LASSO",
      algo == "rf" ~ "Random Forest",
      algo == "svm" ~ "SVM"
    ),
    fsel = dplyr::case_when(
      fsel == "info" ~ "Information Gain",
      fsel == "pca" ~ "PCA"
    )
  ) %>% 
  dplyr::select(fsel, algo, metric = .metric, mean, std_err) %>% 
  dplyr::arrange(desc(mean))

files_cv_res
```

Table above shows the mean 5-fold cross-validation AUC for various ML modelling combinations. Standard error for all models build on information gain selected features is lower compared to PCA feature extration. AUC for information gain based models consistently performs better than their counterparts built using PCA. SVM and Logistic Regression using Lasso are the best performing models with low standard error.


```{r echo=FALSE}
## compile AUC data
files_auc <- list.files(
  path = here::here("analysis/02_ML_models"), pattern = "*.auc.tsv", full.names = TRUE
)

auc_data <- purrr::map_dfr(
  .x = files_auc,
  .f = readr::read_tsv,
  progress = FALSE, show_col_types = FALSE
) %>% 
  tidyr::separate(
    col = model, into = c("fsel", "algo"), sep = " \\+ ", remove = FALSE
  )

class(auc_data) <- c("roc_df", class(auc_data))


ggplot(
  data = auc_data,
  mapping = aes(x = 1 - specificity, y = sensitivity, col = algo, linetype = fsel)
) +
  geom_path(lwd = 1, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(name = "ML method", option = "plasma", end = .6) +
  scale_linetype_manual(name = "Feature selection", values = c(1, 12)) +
  pt_theme

```

Figure above shows the AUC for all 6 modelling combinations and confirms the results. 

#### Final evaluation on hold-out data

Eran et. al. built and evaluated their classification model using 5-fold cross validation. However, an independent evaluation using hold-out dataset was not performed. In our case, two feature selection/extraction methods were combined with three ML methods. All of these modelling combinations were optimized on same cross-validation dataset. As a final step, the best performing model from each of the combinations is evaluated on the hold-out dataset.

```{r echo=FALSE}
## compile final model fit metric
final_fit <- tibble::tibble(
  files = list.files(
    path = here::here("analysis/02_ML_models"),
    pattern = "*.final_prediction.tsv", full.names = TRUE
  )
) %>% 
  dplyr::mutate(
    id = stringr::str_replace(
      string = basename(files), pattern = ".final_prediction.tsv", replacement = "")
  ) %>% 
  purrr::pmap_dfr(
    # .x = files, .y = id,
    .f = function(files, id){
      readr::read_tsv(file = files, progress = FALSE, show_col_types = FALSE) %>% 
        dplyr::mutate(id = id)
    }
  ) %>% 
  dplyr::select(-.estimator, -.config) %>% 
  tidyr::pivot_wider(
    names_from = ".metric",
    values_from = ".estimate"
  ) %>% 
  tidyr::separate(col = id, into = c("algo", "fsel"), sep = "_") %>% 
  dplyr::mutate(
    algo = dplyr::case_when(
      algo == "lasso" ~ "LASSO",
      algo == "rf" ~ "Random Forest",
      algo == "svm" ~ "SVM"
    ),
    fsel = dplyr::case_when(
      fsel == "info" ~ "Information Gain",
      fsel == "pca" ~ "PCA"
    )
  ) %>% 
  dplyr::arrange(desc(roc_auc))

final_fit
```

Interestingly, PCA feature extraction with LASSO shows best AUC. Model generated using information gain feature selection in combination with SVM was best in cross-validation results. However, this combination performed second best on the hold-out dataset.


Overall, two models performed close to Eran et. al. model generated using combination of random forest with lasso regularized regression. 
